{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " Computational Statistics | Summer 2020, M.Sc. in Economics, University of Bonn | [Sona Verdiyeva](https://github.com/s6soverd)\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Bias and Variance tradeoff in Machine Learning](#bias)\n",
    "* [2. What is meant by \"high-dimensional\" and why OLS fails in estimating the parameters in such problems](#high-dim)\n",
    "* [3. LASSO doesn't have a closed form solution, or does it?](#solution)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Bias and Variance tradeoff in Machine Learning <a class=\"anchor\" id=\"bias\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inability of a machine learning method to capture the true relationship between the dependent and independent variables is called **bias**. Given two methods, we can compare how much bias each has, by calculating their sum of squared residuals. In other words; $\\sum_{i = 1}^n (y_{i} - \\hat{y})^2$. The formulae is applied for the training set, to which we have fit the model. \n",
    "\n",
    "Using the fitted model, we can calculate predictions for the testing set. And the difference in sum of squared residuals for the training and test set is called **variance**. Thus, a model that overfits the training set is going to have a very low bias, but this low-bias will come at the high cost of variance; meaning less precise predictions for the testing, or rather, new dataset. \n",
    "\n",
    "Having higher variance means that the model is highly variable with respect to how well it is going to make predictions on the new datasets. It might sometimes do well, sometimes do terrible. A model with lower variance and high bias; on the other hand, will consistently give good predictions for different datasets due to having lower variance, but perhaps not great predictions, as it has a higher bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the best algorithm has low bias, meaning, the method accurately models the true relationship, and has low variance, meaning, across different datasets, it consistently produces good predictions. And there are three commonly used Machine learning(also known as ML) methods that try to achieve that; namely:\n",
    "\n",
    " * Regularization\n",
    " * Boosting\n",
    " * Bagging\n",
    " \n",
    "And in what follows, I will solely focus on regularization out of the three above, and more specifically on LASSO(also known as Least Absolute Shrinkage and Selection Operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. What is meant by \"high-dimensional\" and why OLS fails in estimating the parameters in such problems <a class=\"anchor\" id=\"high-dim\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"high-dimensional\" refers to the case where we have more unknown parameters than the data available; mathematically, $p\\gg n$. \n",
    "\n",
    "OLS is helpless in estimating the parameters when the given number of unknown parameters are much higher than the available data points. To be able to estimate the parameters, the n should be at least p + 1. \n",
    "\n",
    "For $p > n$, the linear model, as we know $Y = X*\\beta^0 + \\epsilon$ has no unique solution, and the linear model is under-determined. As a matter of fact, the linear model or the above equation has infinitely many solutions. \n",
    "\n",
    "To illustrate my point, I will use a very simple linear model, the one with the intercept (or rather $\\beta_0$) and the slope (or rather $\\beta_1$). And we have, for simplicity, have just one data point available. Given these, it is apparent that our unknowns(p = 1) are more than the data points (n = 1). As said before, n should be at least p + 1, meaning, 2, in order to calculate the $\\beta_0$ and $\\beta_1$. The model that I suggest falls down to calculating:\n",
    "$Y = \\beta_0 + \\beta_1 * X_1$. \n",
    "From high-school algebra, we know that here $\\beta_1$ corresponds to the slope of the line, namely, *m*; and the $\\beta_0$ corresponds with the y-intercept, namely *b*. So in a way we can write the above model as:\n",
    "$y = b + m*x$. \n",
    "\n",
    "To calculate *m*,  we need to have 2 data points. However, here, we are given only 1 data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nIICQIICQIICQIICQIMD/4WGLTUVKfYAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x = 2, y = 5, xlim = c(0,20),ylim = c(0,20),type = \"p\", col = \"red\", pch = 16, xlab = \"x\", ylab = \"y\")\n",
    "abline(3, 1, col = \"blue\")\n",
    "abline(-1, 3, col = \"red\")\n",
    "abline(13, -4, col = \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is apparent from the above plot, from the one data point, one can draw infinitely many different lines, all having the sum of squared residuals equal to zero; thus, there is no one unique $\\beta_0$ and $\\beta_1$ which minimizes the sum of sqaured residuals. Given this, we have infinitely many solutions; or more pessimistically, OLS doesn't have a solution for this rather simplified \"high-dimensional\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, without any constraints or additional information, it is impossible to find a solution to the above problem. If we are given a constraint, out of many solutions, we can pick the one that satisfies that constraint, hence being the solution to the above problem. \n",
    "\n",
    "In this regard, regularization methods come at handy. Before delving into how LASSO is commonly sought after in problems of high-dimensional nature, I will discuss two assumptions that build the intutition behind LASSO.\n",
    " > Sparsity assumption\n",
    " \n",
    " > Basis Pursuit Linear program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity assumption for an unknown vector means that  that vector has less number of non-zero elements; hence the name sparse. So the sparsity assumption for the true $\\beta$ is a constraint that results in beta having less number of non-zero elements. One way of doing this is to use *$l_0$*-norm. For a quick brush-up, the *$l_0$*-norm, *$l_1$*-norm and *$l_2$*-norm are defined as:\n",
    "\n",
    "---\n",
    "$$\\| \\beta \\|_0 = \\sum_{j = 1}^{p} I(\\beta_{j} \\neq 0) \\\\[1pt]\n",
    "\\| \\beta \\|_1 = \\sum_{j = 1}^{p} |\\beta_{j}| \\\\[1pt]\n",
    "\\| \\beta \\|_2 = \\sum_{j = 1}^{p} \\beta_{j}^2 \\\\[1pt] \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our *$l_{0}$*-norm constrained optimization looks like this:\n",
    "\n",
    "---\n",
    "$$\\underset{\\beta \\in \\mathbb{R}}{\\text{minimize}} \\\n",
    "\\| \\beta \\|_0 \\\\\n",
    "\\text{subject to} \\\n",
    " Y = X\\beta \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above constrained optimization problem is like a best subset selection. However, when our p is quite large, then we need to look at $\\binom{p}{s}$ number of subsets, where $s \\leq n$, which is computationally infeasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Basis Pursuit Linear Program* uses *$l_1$*-norm to solve the equation:\n",
    "\n",
    "---\n",
    "$$\\underset{\\beta \\in \\mathbb{R}}{\\text{minimize}} \\\n",
    "\\| \\beta \\|_1 \\\\\n",
    "\\text{subject to} \\\n",
    " Y = X\\beta \\\\\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using a simplified example (Niharika,. G., (2018)), we can show how under sparsity assumption, one can solve under-determined linear equation. Suppose we have the below equation:\n",
    "$$ \\begin{bmatrix}\n",
    "       2\\\\[0.3em]\n",
    "       2 \\\\[0.3em]\n",
    "     \\end{bmatrix} = \\begin{bmatrix} \n",
    "                             2 & 1 & 0 \\\\[0.3em]\n",
    "                             2 & 0 & 1 \\\\[0.3em]\n",
    "                      \\end{bmatrix}  \\times   \\begin{bmatrix} \n",
    "                                              \\beta_1 \\\\[0.3em]\n",
    "                                              \\beta_2 \\\\[0.3em]\n",
    "                                              \\beta_3 \\\\[0.3em]\n",
    "                                             \\end{bmatrix}\n",
    "$$\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the $\\beta$ has a sparse solution. To find the solution, we set certain components of the subset of $\\beta$ to zero such that equation holds. Here are some solutions:\n",
    "$$[\\beta_1 = 1, \\: \\beta_2 = 0, \\; \\beta_3 = 0], \\; \\| \\beta \\|_0 = 1, \\; \\| \\beta \\|_1 = 1 \\\\\n",
    "[\\beta_1 = 0, \\: \\beta_2 = 2, \\; \\beta_3 = 2], \\; \\| \\beta \\|_0 = 2, \\; \\| \\beta \\|_1 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution with the least number of non-zero elements is $[\\beta_1 = 1, \\: \\beta_2 = 0, \\; \\beta_3 = 0]$, since $\\| \\beta \\|_0 = 1 < \\| \\beta \\|_0 = 2$. And the solution of Basis Pursuit Linear Program is also the same for this particular equation; meaning $[\\beta_1 = 1, \\: \\beta_2 = 0, \\; \\beta_3 = 0]$, since $\\| \\beta \\|_1 = 1 < \\| \\beta \\|_1 = 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proceeding section, I will discuss how shrinkage of coeffiecients, whose intution we built throughout this section, helps in finding a solution to high-dimensional problems, and LASSO's central role in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. LASSO doesn't have a closed form solution, or does it? <a class=\"anchor\" id=\"solution\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO is a *$l_1$*-regularized regression method. It estimates the regression parameters by solving the below constrained optimization problem:\n",
    "\n",
    "---\n",
    "$$\\underset{\\beta \\in \\mathbb{R^{p}}}{\\text{minimize}} \\\n",
    "\\frac{1}{n}\\| Y - X\\beta \\|_2^2 \\\\\n",
    "\\text{subject to} \\\\\n",
    "\\| \\beta \\|_1 \\leq t\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $t$ is a upper-bound over $l_1$-norm. It is obvious from the inequality that if our $t \\geq \\hat{\\beta}^{OLS}$, then our LASSO estimate and OLS estimate are the same. Since OLS-estimated beta minimizes the sum of squared residuals and at the same time, satisfies the constraint as well. If our $t \\leq \\hat{\\beta}^{OLS}$, then LASSO will shrink the coefficients towards zero, or even set some to zero. \n",
    "\n",
    "The Lagrange function corresponding to the above constrained regression optimization is given below. For a quick note, Langrangian  of an optimization problem is the sum of objective function and its weighted constraints.\n",
    "\n",
    "---\n",
    "$$\\hat{\\beta(\\lambda)} = \\underset{\\beta \\in \\mathbb{R^{p}}}{\\text{arg min}} \\\n",
    "\\frac{1}{n}\\| Y - X\\beta \\|_2^2 + \\lambda\\| \\beta \\|_1 \\\\\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda \\geq 0$ is a parameter that determines the intensity of shrinkage. Because the objective function above is not differentiable ($|x|$ has no derivative at 0, hence, not differentiable, same hold for $\\| \\beta \\|_1 = | \\beta |$), the LASSO has generally no closed form solution. However, in the case of single-variable model and orthonormal design matrix (basically, for each $i \\neq j,  \\: X_{i}^T X_{j} = 0$, meaning the variables are uncorrelated), a solution is derivable. \n",
    "In the paragraphs that follow, we will look at how to derive a LASSO solution for the single-variable case and why for multi-variable models, this doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Linear Regression** \n",
    "\n",
    "Assume we have one predictor, $p = 1$, and also our predictor is standardized ($Y = X_1\\beta_1 + \\epsilon$). Then, the optimization problem is reduced down to finding the $\\hat{\\beta}^{LASSO}$ that is the solution to $\\underset{\\beta \\in \\mathbb{R^{p}}}{\\text{minimize}} \\\n",
    "\\frac{1}{n}\\| Y - X_1\\beta_1 \\|_2^2 + \\lambda| \\beta_1 |$. Then, from the Stationarity Condition of the Kuhn-Tucker, sub-differential of the Lagrangian function at the $\\hat{\\beta}^{LASSO}$ should contain zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-gradients of a convex function $f$ at $x$ is called subdifferential of $f$ at $x$, and is denoted as $sign(x)$. Subdifferential of $f = |x|$ is equal to:\n",
    "$$\n",
    "sign(x) =\n",
    "  \\begin{cases}\n",
    "    -1       & \\quad \\text{if } x < 0 \\\\\n",
    "    [-1, 1]  & \\quad \\text{if } x = 0 \\\\\n",
    "    1        & \\quad \\text{if } x > 0 \\\\\n",
    "  \\end{cases}\n",
    "$$\n",
    "  \n",
    "\n",
    "\n",
    "By the same token, sub-differential of $|\\beta_1|$ is equal to:\n",
    "$$\n",
    "sign(\\beta_1) = \n",
    "   \\begin{cases}\n",
    "    -1       & \\quad \\text{if } \\beta_1 < 0 \\\\\n",
    "    [-1, 1]  & \\quad \\text{if } \\beta_1 = 0 \\\\\n",
    "    1        & \\quad \\text{if } \\beta_1 > 0 \\\\\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the sub-differential of the above Lagrangian:\n",
    "$$\n",
    "0 \\in -\\frac{2}{n}X_1^T(Y - X_1\\hat{\\beta_1}) + \\lambda sign(\\hat{\\beta_1})\n",
    "$$\n",
    "\n",
    "\n",
    "**Note**:\n",
    "\n",
    "Taking subdifferential of a differentiable function means taking its derivative.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{2}{n}X_1^T(Y - X_1\\hat{\\beta_1}) + \\lambda sign(\\hat{\\beta_1}) = 0 \\\\\n",
    "\\frac{1}{n}X_1^T(Y - X_1\\hat{\\beta_1}) = \\frac{\\lambda}{2} sign(\\hat{\\beta_1})\n",
    "$$\n",
    "As our predictor is standardized, $\\frac{1}{n}X_1^T X_1 = 1$\n",
    "\n",
    "To show how:\n",
    "\n",
    "---\n",
    "$$\n",
    "X_1^T = \\Bigg(\\frac{x_{1_1} - \\frac{x_{1_1}+\\dotsb+x_{1_n}}{n}}{\\sqrt[2]{\\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{n}}}  \\cdots \\frac{x_{1_n} - \\frac{x_{1_1}+\\dotsb+x_{1_n}}{n}}{\\sqrt[2]{\\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{n}}} \\Bigg) \\\\\n",
    "X_1 = \\begin{pmatrix} \n",
    "         \\frac{x_{1_1} - \\frac{x_{1_1}+\\dotsb+x_{1_n}}{n}}{\\sqrt[2]{\\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{n}}} \\\\\n",
    "         \\vdots \\\\\n",
    "         \\frac{x_{1_n} - \\frac{x_{1_1}+\\dotsb+x_{1_n}}{n}}{\\sqrt[2]{\\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{n}}} \\\\\n",
    "       \\end{pmatrix}        \n",
    "$$\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data point of the variable $X_1$ we deduct the mean and divide by the standard deviation of the variable $X_1$ (so as to standardize the $X_1$). \n",
    "\n",
    "Multiplying $X_1^T X_1$, we get:\n",
    "$$ X_1^T X_1 = \\frac{(x_{1_1} - \\bar{x_1})^2 +\\dotsb+(x_{1_n} - \\bar{x_1})^2}{\\sqrt[2]{\\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{n}}} = \\frac{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2}{\\sum_{i = 1}^{n} (x_{1_i} - \\bar{x_1})^2} \\times n = n\n",
    "$$\n",
    "\n",
    "Hence, $\\frac{1}{n} X_1^T X_1 = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
