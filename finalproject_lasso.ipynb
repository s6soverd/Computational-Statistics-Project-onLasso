{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " Computational Statistics | Summer 2020, M.Sc. in Economics, University of Bonn | [Sona Verdiyeva](https://github.com/s6soverd)\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Bias and Variance tradeoff in Machine Learning](#bias)\n",
    "* [2. What is meant by \"high-dimensional\" and why OLS fails in estimating the parameters in such problems](#high-dim)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Bias and Variance tradeoff in Machine Learning <a class=\"anchor\" id=\"bias\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inability of a machine learning method to capture the true relationship between the dependent and independent variables is called **bias**. Given two methods, we can compare how much bias each has, by calculating their sum of squared residuals. In other words; $\\sum_{i = 1}^n (y_{i} - \\hat{y})^2$. The formulae is applied for the training set, to which we have fit the model. \\\n",
    "Using the fitted model, we can calculate predictions for the testing set. And the difference in sum of squared residuals for the training and test set is called **variance**. Thus, a model that overfits the training set is going to have a very low bias, but this low-bias will come at the high cost of variance; meaning less precise predictions for the testing, or rather, new dataset. \\\n",
    "Having higher variance means that the model is highly variable with respect to how well it is going to make predictions on the new datasets. It might sometimes do well, sometimes do terrible. A model with lower variance and high bias; on the other hand, will consistently give good predictions for different datasets due to having lower variance, but perhaps not great predictions, as it has a higher bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the best algorithm has low bias, meaning, the method accurately models the true relationship, and has low variance, meaning, across different datasets, it consistently produces good predictions. And there are three commonly used Machine learning(also known as ML) methods that try to achieve that; namely:\n",
    "\n",
    " * Regularization\n",
    " * Boosting\n",
    " * Bagging\n",
    " \n",
    "And in what follows, I will solely focus on regularization out of the three above, and more specifically on LASSO(also known as Least Absolute Shrinkage and Selection Operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. What is meant by \"high-dimensional\" and why OLS fails in estimating the parameters in such problems <a class=\"anchor\" id=\"high-dim\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"high-dimensional\" refers to the case where we have more unknown parameters than the data available; mathematically, $p\\gg n$. \\\n",
    "OLS is helpless in estimating the parameters when the given number of unknown parameters are much higher than the available data points. To be able to estimate the parameters, the n should be at least p + 1. \n",
    "For $p > n$, the linear model, as we know $Y = X*\\beta^0 + \\epsilon$ has no unique solution, and the linear model is under-determined. As a matter of fact, the linear model or the above equation has infinitely many solutions. \\\n",
    "To illustrate my point, I will use a very simple linear model, the one with the intercept (or rather $\\beta_0$) and the slope (or rather $\\beta_1$). And we have, for simplicity, have just one data point available. Given these, it is apparent that our unknowns(p = 1) are more than the data points (n = 1). As said before, n should be at least p + 1, meaning, 2, in order to calculate the $\\beta_0$ and $\\beta_1$. The model that I suggest falls down to calculating:\n",
    "$Y = \\beta_0 + \\beta_1 * X_1$. \n",
    "From high-school algebra, we know that here $\\beta_1$ corresponds to the slope of the line, namely, *m*; and the $\\beta_0$ corresponds with the y-intercept, namely *b*. So in a way we can write the above model as:\n",
    "$y = b + m*x$. \\\n",
    "To calculate *m*,  we need to have 2 data points. However, here, we are given only 1 data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nIICQIICQIICQIICQIMD/4WGLTUVKfYAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x = 2, y = 5, xlim = c(0,20),ylim = c(0,20),type = \"p\", col = \"red\", pch = 16, xlab = \"x\", ylab = \"y\")\n",
    "abline(3, 1, col = \"blue\")\n",
    "abline(-1, 3, col = \"red\")\n",
    "abline(13, -4, col = \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is apparent from the above plot, from the one data point, one can draw infinitely many different lines, all having the sum of squared residuals equal to zero; thus, there is no one unique $\\beta_0$ and $\\beta_1$ which minimizes the sum of sqaured residuals. Given this, we have infinitely many solutions; or more pessimistically, OLS doesn't have a solution for this rather simplified \"high-dimensional\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
